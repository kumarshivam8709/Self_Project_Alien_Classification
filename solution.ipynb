{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Loading dataset\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 1: Preprocessing the data\n",
        "\n",
        "# Encoding the 'tail' column (yes -> 1, no -> 0)\n",
        "df['tail_encoded'] = df['tail'].map({'yes': 1, 'no': 0})\n",
        "\n",
        "# Encoding the 'species' column using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['species_encoded'] = label_encoder.fit_transform(df['species'])\n",
        "\n",
        "# Standardizing the 'fingers' column\n",
        "scaler = StandardScaler()\n",
        "df['fingers_scaled'] = scaler.fit_transform(df[['fingers']])\n",
        "\n",
        "# Converting 'message' column to TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(df['message']).toarray()\n",
        "\n",
        "# Combining all features into a single DataFrame\n",
        "X = pd.DataFrame(tfidf_features, columns=[f'tfidf_{i}' for i in range(tfidf_features.shape[1])])\n",
        "X['tail_encoded'] = df['tail_encoded']\n",
        "X['fingers_scaled'] = df['fingers_scaled']\n",
        "\n",
        "# Target variable\n",
        "y = df['species_encoded']\n",
        "\n",
        "# Splitting the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Train Logistic Regression model\n",
        "logreg_model = LogisticRegression(max_iter=1000)\n",
        "logreg_model.fit(X_train, y_train)\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# BERT Model Definition\n",
        "class CustomBERTModel(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_labels, dropout_rate):\n",
        "        super(CustomBERTModel, self).__init__()\n",
        "        self.bert = BertForSequenceClassification.from_pretrained(bert_model_name, num_labels=num_labels)\n",
        "\n",
        "        self.fc1 = nn.Linear(2, 16)  # 2 features: tail and fingers\n",
        "        self.dropout_fc1 = nn.Dropout(p=dropout_rate)\n",
        "        self.fc2 = nn.Linear(768 + 16, 256)  # 768 (BERT output) + 16 (tabular data output from fc1)\n",
        "        self.dropout_fc2 = nn.Dropout(p=dropout_rate)\n",
        "        self.fc3 = nn.Linear(256, num_labels)\n",
        "        self.dropout_fc3 = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, tail_encoded, fingers_scaled):\n",
        "        bert_outputs = self.bert.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = bert_outputs[1]  # CLS token output (768 features)\n",
        "\n",
        "        tabular_data = torch.cat((tail_encoded.unsqueeze(1), fingers_scaled.unsqueeze(1)), dim=1)\n",
        "        tabular_features = torch.relu(self.fc1(tabular_data))\n",
        "        tabular_features = self.dropout_fc1(tabular_features)\n",
        "\n",
        "        combined = torch.cat((pooled_output, tabular_features), dim=1)  # Shape will be 768 (BERT) + 16 (tabular)\n",
        "        combined = torch.relu(self.fc2(combined))  # Input to fc2 is now 768 + 16 = 784\n",
        "        combined = self.dropout_fc2(combined)\n",
        "\n",
        "        logits = self.fc3(combined)\n",
        "        logits = self.dropout_fc3(logits)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Initialize BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = CustomBERTModel('bert-base-uncased', num_labels=len(df['species'].unique()), dropout_rate=0.3)\n",
        "\n",
        "# Tokenize the text data for BERT\n",
        "train_tokenized_inputs = tokenizer(\n",
        "    df.loc[X_train.index, 'message'].tolist(),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=32,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "test_tokenized_inputs = tokenizer(\n",
        "    df.loc[X_test.index, 'message'].tolist(),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=32,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "# Converting other features and labels to tensors\n",
        "train_tail_encoded = torch.tensor(df.loc[X_train.index, 'tail_encoded'].tolist(), dtype=torch.float32)\n",
        "train_fingers_scaled = torch.tensor(df.loc[X_train.index, 'fingers_scaled'].tolist(), dtype=torch.float32)\n",
        "train_labels = torch.tensor(y_train.tolist(), dtype=torch.long)\n",
        "\n",
        "test_tail_encoded = torch.tensor(df.loc[X_test.index, 'tail_encoded'].tolist(), dtype=torch.float32)\n",
        "test_fingers_scaled = torch.tensor(df.loc[X_test.index, 'fingers_scaled'].tolist(), dtype=torch.float32)\n",
        "test_labels = torch.tensor(y_test.tolist(), dtype=torch.long)\n",
        "\n",
        "# Creating DataLoader for BERT\n",
        "train_dataset = TensorDataset(\n",
        "    train_tokenized_inputs['input_ids'], train_tokenized_inputs['attention_mask'],\n",
        "    train_tail_encoded, train_fingers_scaled, train_labels\n",
        ")\n",
        "test_dataset = TensorDataset(\n",
        "    test_tokenized_inputs['input_ids'], test_tokenized_inputs['attention_mask'],\n",
        "    test_tail_encoded, test_fingers_scaled, test_labels\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Training the BERT model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bert_model.to(device)\n",
        "optimizer = Adam(bert_model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 10\n",
        "bert_model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids, attention_mask, tail_encoded, fingers_scaled, labels = [x.to(device) for x in batch]\n",
        "        optimizer.zero_grad()\n",
        "        logits = bert_model(input_ids, attention_mask, tail_encoded, fingers_scaled)\n",
        "        loss = loss_fn(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}\")\n",
        "\n",
        "# Making predictions with BERT\n",
        "bert_model.eval()\n",
        "bert_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, attention_mask, tail_encoded, fingers_scaled, labels = [x.to(device) for x in batch]\n",
        "        logits = bert_model(input_ids, attention_mask, tail_encoded, fingers_scaled)\n",
        "        bert_preds.append(torch.argmax(logits, dim=-1).cpu().numpy())\n",
        "\n",
        "bert_preds = np.concatenate(bert_preds)\n",
        "\n",
        "# Step 4: Voting Classifier (BERT + Logistic Regression + Random Forest)\n",
        "# We have predictions from BERT, Logistic Regression, and Random Forest\n",
        "\n",
        "# Logistic Regression predictions\n",
        "logreg_preds = logreg_model.predict(X_test)\n",
        "\n",
        "# Random Forest predictions\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "\n",
        "# Voting Function\n",
        "def majority_vote(bert_preds, logreg_preds, rf_preds):\n",
        "    preds = np.stack([bert_preds, logreg_preds, rf_preds], axis=1)\n",
        "    final_preds = [np.bincount(row).argmax() for row in preds]\n",
        "    return np.array(final_preds)\n",
        "\n",
        "# Combining predictions via majority voting\n",
        "final_preds = majority_vote(bert_preds, logreg_preds, rf_preds)\n",
        "\n",
        "# Evaluating the final predictions\n",
        "final_accuracy = accuracy_score(y_test, final_preds)\n",
        "print(f\"Final Ensemble Accuracy: {final_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CE-RRuEPU490",
        "outputId": "ce5ff4a1-2c95-438c-ba38-fd8bb317773d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.2592720127105714\n",
            "Epoch 2, Loss: 1.9062764167785644\n",
            "Epoch 3, Loss: 1.6359659790992738\n",
            "Epoch 4, Loss: 1.4986241936683655\n",
            "Epoch 5, Loss: 1.448446787595749\n",
            "Epoch 6, Loss: 1.3838802039623261\n",
            "Epoch 7, Loss: 1.293114334344864\n",
            "Epoch 8, Loss: 1.3277151918411254\n",
            "Epoch 9, Loss: 1.2641692924499512\n",
            "Epoch 10, Loss: 1.2238049006462097\n",
            "Final Ensemble Accuracy: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the test dataset\n",
        "new_df = pd.read_csv('/content/test.csv')\n",
        "\n",
        "# Step 1: Preprocessing the new data\n",
        "# Encoding the 'tail' column (yes -> 1, no -> 0)\n",
        "new_df['tail_encoded'] = new_df['tail'].map({'yes': 1, 'no': 0})\n",
        "\n",
        "\n",
        "# Standardizimg the 'fingers' column\n",
        "new_df['fingers_scaled'] = scaler.transform(new_df[['fingers']])\n",
        "\n",
        "# Converting 'message' column to TF-IDF features using the same TfidfVectorizer\n",
        "new_tfidf_features = tfidf_vectorizer.transform(new_df['message']).toarray()\n",
        "\n",
        "# Combining all features into a single DataFrame for the new data\n",
        "X_new = pd.DataFrame(new_tfidf_features, columns=[f'tfidf_{i}' for i in range(new_tfidf_features.shape[1])])\n",
        "X_new['tail_encoded'] = new_df['tail_encoded']\n",
        "X_new['fingers_scaled'] = new_df['fingers_scaled']\n"
      ],
      "metadata": {
        "id": "qVL0P09JuwhM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression predictions for the new data\n",
        "logreg_new_preds = logreg_model.predict(X_new)"
      ],
      "metadata": {
        "id": "LAZF_hGMyhO3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest predictions for the new data\n",
        "rf_new_preds = rf_model.predict(X_new)"
      ],
      "metadata": {
        "id": "YMz501NOymgE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the text data for BERT\n",
        "new_tokenized_inputs = tokenizer(\n",
        "    new_df['message'].tolist(),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=32,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "# Converting the other features (tail_encoded, fingers_scaled) to tensors\n",
        "new_tail_encoded = torch.tensor(new_df['tail_encoded'].tolist(), dtype=torch.float32)\n",
        "new_fingers_scaled = torch.tensor(new_df['fingers_scaled'].tolist(), dtype=torch.float32)\n",
        "\n",
        "# Creating DataLoader for the new data\n",
        "new_dataset = TensorDataset(\n",
        "    new_tokenized_inputs['input_ids'], new_tokenized_inputs['attention_mask'],\n",
        "    new_tail_encoded, new_fingers_scaled\n",
        ")\n",
        "\n",
        "new_loader = DataLoader(new_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# BERT predictions\n",
        "bert_model.eval()\n",
        "bert_new_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in new_loader:\n",
        "        input_ids, attention_mask, tail_encoded, fingers_scaled = [x.to(device) for x in batch]\n",
        "        logits = bert_model(input_ids, attention_mask, tail_encoded, fingers_scaled)\n",
        "        bert_new_preds.append(torch.argmax(logits, dim=-1).cpu().numpy())\n",
        "\n",
        "bert_new_preds = np.concatenate(bert_new_preds)"
      ],
      "metadata": {
        "id": "2KlXGKyVypnW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Voting function (already defined)\n",
        "def majority_vote(bert_preds, logreg_preds, rf_preds):\n",
        "    preds = np.stack([bert_preds, logreg_preds, rf_preds], axis=1)\n",
        "    final_preds = [np.bincount(row).argmax() for row in preds]\n",
        "    return np.array(final_preds)\n",
        "\n",
        "# Applying majority voting for the final predictions\n",
        "final_new_preds = majority_vote(bert_new_preds, logreg_new_preds, rf_new_preds)"
      ],
      "metadata": {
        "id": "yVo3BODAyuKJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the integer predictions back to species names\n",
        "final_new_species = label_encoder.inverse_transform(final_new_preds)\n",
        "\n",
        "# Saving the predictions to a new CSV\n",
        "new_df['predicted_species'] = final_new_species\n",
        "new_df.to_csv('new_predictions.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'new_predictions.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeHJ0P4Myzbj",
        "outputId": "9675477a-01a0-47a5-93a1-4d8c9efbb1e5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to 'new_predictions.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W_BUrlkGy3ep"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}